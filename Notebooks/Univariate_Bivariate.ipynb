{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f14ef8-32b4-41e1-a52f-32593c839101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ee110-d70b-46fb-9f8b-36dd1e2c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ad215-1ae3-4763-a65c-8057734ccee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b8266-d822-4817-9af8-cf55d59e7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "file_path = \"../Data/Cleaned/income2024_cleaned.csv\"\n",
    "\n",
    "data_log_1 = pd.read_csv(file_path)\n",
    "display(data_log_1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a80bdb-b0c0-41ba-959b-eb9ce2f97f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring that the label gets a string value instead of a numeric value\n",
    "label_mapping = {\n",
    "    -1: \"Early Delivery\",\n",
    "    0: \"On-time Delivery\",\n",
    "    1: \"Delayed Delivery\"\n",
    "}\n",
    "data_log_1[\"label\"] = data_log_1[\"label\"].map(label_mapping)\n",
    "data_log_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65088f-a795-45dd-a467-2490a1bd22c3",
   "metadata": {},
   "source": [
    "## UNIVARIATE ANALYSIS ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cfedb-9a47-4f3e-a01d-7768fe9d3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop specific columns : The customer_\n",
    "data_log_2 = data_log_1.drop(columns=[\"shipping_daet\", \"oder_date\"], errors ='ignore')\n",
    "display(data_log_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2294e-1478-40a3-9643-effea4dc5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate numerical and categorical columns\n",
    "num_cols = data_log_2.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = data_log_2.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5b377-3a92-4809-9d41-07d7bd6f7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical data\n",
    "print(\"\\nSummary statistics for numerical data: \")\n",
    "display(data_log_2[num_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439fc58-acf9-4306-87e5-6cd6666555b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for categorical data\n",
    "print(\"\\nValue counts for categorical data: \")\n",
    "for col in cat_cols: \n",
    "    print(f\"\\n{col}:\\n\", data_log_2[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418821da-b344-447b-8c2f-7e658b10f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for numerical features\n",
    "# Histograms for numerical features\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data_log_1[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9bbe5-74ed-4d7e-acb5-8c71d67af28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots for numerical features: check for outliers\n",
    "for col in num_cols:\n",
    "    plt.figure9figsize=(10, 6)\n",
    "    sns.boxplot(x=data_log_1[col])\n",
    "    plt.title(f\"{col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b1506-0a59-4e83-8134-70cab051faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    data_log_1[col].value_counts().nlargest(10).plot(kind='bar', color='skyblue')\n",
    "    plt.title(f\"{col}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c65b4d-d211-4299-b726-5d3adf4e615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does the number of orders differ by shipping mode \n",
    "temp=data_log_2['shipping_mode'].value_counts()\n",
    "plt.figure(figsize=(6,6))\n",
    "ax=sns.barplot(x=temp.index,y=temp.values,palette='viridis')\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "#ax.set_xticklabels([label.replace(' ', '\\n') for label in data['customer_segment'].unique()])\n",
    "\n",
    "plt.xlabel(\"shipping_mode\")\n",
    "plt.ylabel(\"Count of orders\")\n",
    "plt.title(\"Number of orders per shipping_mode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b044980-3457-474d-9529-0ede2dfc2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure 'order_date' is in datetime format\n",
    "data_log_2['order_date'] = pd.to_datetime(data_log_2['order_date'], errors='coerce')\n",
    "\n",
    "# Copy the dataset and extract the year from 'order_date'\n",
    "temp = data_log_2.copy()\n",
    "temp['year'] = temp['order_date'].dt.year\n",
    "\n",
    "# Define categorical and numerical columns for analysis\n",
    "categories = ['market', 'customer_segment', 'department_id', 'category_id']\n",
    "numeric_metrics = ['order_item_quantity', 'sales', 'profit_per_order']\n",
    "\n",
    "# Group data by category and calculate total sales\n",
    "total_sales_by_category = temp.groupby(categories)['sales'].sum().reset_index()\n",
    "print(total_sales_by_category)\n",
    "\n",
    "# Plot total sales by category using a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='market', y='sales', hue='customer_segment', data=total_sales_by_category)\n",
    "plt.title(\"Total Sales by Market and Customer Segment\")\n",
    "plt.xlabel(\"Market\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.legend(title=\"Customer Segment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "for category in categories:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a row of 3 subplots\n",
    "    \n",
    "    if category in ['department_id', 'category_id']:\n",
    "        for idx, metric in enumerate(numeric_metrics):\n",
    "            # Find the top 4 departments/categories based on total metric value\n",
    "            top_values = temp.groupby(category)[metric].sum().nlargest(4).index\n",
    "            \n",
    "            # Filter dataset to include only top 4 departments/categories\n",
    "            filtered_temp = temp[temp[category].isin(top_values)]\n",
    "            \n",
    "            # Group by category and year, then plot the data\n",
    "            grouped_data = filtered_temp.groupby([category, 'year'])[metric].sum().unstack(category)\n",
    "            grouped_data.plot(kind='bar', ax=axes[idx])\n",
    "            \n",
    "            axes[idx].set_title(f'Yearly {metric} for Top 4 {category}s')\n",
    "            axes[idx].set_ylabel(f'Total {metric}')\n",
    "    \n",
    "    else:\n",
    "        for idx, metric in enumerate(numeric_metrics):\n",
    "            # Group by category and year, then plot the data\n",
    "            grouped_data = temp.groupby([category, 'year'])[metric].sum().unstack(category)\n",
    "            grouped_data.plot(kind='bar', ax=axes[idx])\n",
    "            \n",
    "            axes[idx].set_title(f'Yearly {metric} per {category}')\n",
    "            axes[idx].set_ylabel(f'Total {metric}')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05304776-d855-48ca-ba58-4f07804b0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Analyze sales distribution across different countries\n",
    "temp = data_log_2.copy()\n",
    "\n",
    "# Group sales data by country and sort in descending order\n",
    "top_countries = temp.groupby('order_country')['sales'].sum().reset_index()\n",
    "top_countries = top_countries.sort_values(by='sales', ascending=False).head(20)\n",
    "\n",
    "# Create a bar plot to visualize total sales by country\n",
    "plt.figure(figsize=(19, 6))\n",
    "ax = sns.barplot(data=top_countries, x='order_country', y='sales', palette='viridis')\n",
    "\n",
    "# Annotate bars with sales values\n",
    "for bar in ax.patches:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{int(height)}', (bar.get_x() + bar.get_width() / 2., height),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# Format x-axis labels for better readability\n",
    "ax.set_xticklabels([label.replace(' ', '\\n') for label in top_countries['order_country'].unique()])\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Total Sales by Country\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fab958-db2c-4c89-be80-56f312bbf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group by department and calculate total sales\n",
    "department_sales = data_log_2.groupby('department_name')['sales'].sum().reset_index()\n",
    "\n",
    "# Sort by sales in descending order\n",
    "department_sales = department_sales.sort_values(by='sales', ascending=False)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(18.5, 6))\n",
    "ax = sns.barplot(data=department_sales, x='department_name', y='sales', palette='viridis')\n",
    "\n",
    "# Annotate bars with total sales values\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{int(height)}', \n",
    "                (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# Format x-axis labels for better readability\n",
    "ax.set_xticklabels([label.replace(' ', '\\n') for label in department_sales['department_name']])\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Department\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Total Sales by Department\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73050561-3b4b-4b70-9081-d90224da8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data = data_log_2[[\"department_name\", \n",
    "                    \"latitude\",\n",
    "                    \"longitude\",\n",
    "                    \"sales\",\n",
    "                    \"profit_per_order\",\n",
    "                    \"category_name\",\n",
    "                    \"order_item_discount\",\n",
    "                    \"order_item_discount_rate\",\n",
    "                    \"order_item_product_price\",\n",
    "                    \"order_item_quantity\",\n",
    "                    \"order_item_total_amount\",\n",
    "                    \"order_profit_per_order\",\n",
    "                    \"order_city\", \n",
    "                    \"order_country\",\n",
    "                    \"order_status\",\n",
    "                    \"product_name\",\n",
    "                    \"product_price\",\n",
    "                    \"shipping_mode\",\n",
    "                    \"label\"\n",
    "                   ]]\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab17300-d985-4618-ac33-151a17e382ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing store locations according to number of purchases \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851736e8-fe32-4fed-90d4-ba916a028b87",
   "metadata": {},
   "source": [
    "## BIVARIATE ANALYSIS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f703aa-e73c-4af0-ab90-8a3d11fcf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables to compare\n",
    "cat_var = [('customer_segment', 'order_status'), ('payment_type', 'order_status')]\n",
    "\n",
    "for var1, var2 in cat_var:\n",
    "    crosstab = pd.crosstab(data_log_2[var1], data_log_2[var2])\n",
    "    chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "print(f\"\\nChi-Square test for {var1} vs {var2}\")\n",
    "print(f\"Chi-square statistics: {chi2}, p-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d288ed0-e8ec-4c30-a817-910b972c9ea7",
   "metadata": {},
   "source": [
    "* This test indicates that there is a strong dependency between payment_type and order_status which suggest that order status is strongly affected by the payment type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858938d4-c824-4530-9c8e-af7865545ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for visualization\n",
    "plt.figure(figsize=(10,2))\n",
    "sns.boxplot(x=data_log_2[\"customer_segment\"], y=data_log_2[\"sales_per_customer\"])\n",
    "plt.title(\"Sales per Customer by Segment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ANOVA Test \n",
    "segments = data_log_2[\"customer_segment\"].unique()\n",
    "groups = [data_log_2[data_log_2[\"customer_segment\"] == seg][\"sales_per_customer\"].dropna() for seg in segments]\n",
    "anova_result = f_oneway(*groups)\n",
    "\n",
    "print(f\"ANOVA Test for Customer Segment vs Sales per Customer\")\n",
    "print(f\"F-Statistic: {anova_result.statistic}, P-Value: {anova_result.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bd45c-b08f-4aa3-a15d-91201277e8a0",
   "metadata": {},
   "source": [
    "* There is no statistically significant difference in sales_per_customer across different customer_segments\n",
    "* The differences in sales across customer segments are as a result of random varaition \n",
    "* This suggest that customer segment does not strongly influence sales per customer in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cc61e-be49-45ea-9a0e-69440a81e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=data_log_2[\"order_item_product_price\"], y=data_log_2[\"order_item_quantity\"])\n",
    "plt.title(\"Product Price vs Order Quantity\")\n",
    "plt.show()\n",
    "\n",
    "# Pearson Correlation\n",
    "corr, p_value = stats.pearsonr(data_log_2[\"order_item_product_price\"], data_log_2[\"order_item_quantity\"])\n",
    "print(f\"Pearson Correlation: {corr}, P-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b98b8f-8442-4e16-97e3-2e2abe988e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables to compare with delivery label\n",
    "cat_vars = [('label', 'shipping_mode'), ('label', 'order_status')]\n",
    "\n",
    "for var1, var2 in cat_vars:\n",
    "    crosstab = pd.crosstab(data_log_2[var1], data_log_2[var2])\n",
    "    chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "    \n",
    "    print(f\"\\nChi-Square Test for {var1} vs {var2}\")\n",
    "    print(f\"Chi-Square Statistic: {chi2}, P-Value: {p}\")\n",
    "    \n",
    "    # Visualize with heatmap\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.heatmap(crosstab, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "    plt.title(f\"Heatmap of {var1} vs {var2}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d7549-866c-4497-a573-63ecde72906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=data_log_2[\"order_item_total_amount\"], y=data_log_2[\"order_item_discount\"], hue=data_log_2[\"label\"])\n",
    "plt.title(\"Total Order Amount vs Discount by Delivery Status\")\n",
    "plt.show()\n",
    "\n",
    "# Pearson Correlation\n",
    "corr, p_value = stats.pearsonr(data_log_2[\"order_item_total_amount\"], data_log_2[\"order_item_discount\"])\n",
    "print(f\"Pearson Correlation: {corr}, P-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524efa78-193b-414d-a57a-b01377133a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load dataset\n",
    "df = data_log_2 # Ensure the dataset is preloaded\n",
    "\n",
    "# Convert categorical variables to string type\n",
    "df[\"label\"] = df[\"label\"].astype(str)\n",
    "df[\"shipping_mode\"] = df[\"shipping_mode\"].astype(str)\n",
    "df[\"latitude\"] = df[\"latitude\"].astype(str)\n",
    "df[\"longitude\"] = df[\"longitude\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd3d35-0b46-4ff4-b7e1-a0d9d77f8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edfa92-e216-4995-8964-5f686b0ed39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_processing_time\"] = (pd.to_datetime(df[\"shipping_date_only\"]) - pd.to_datetime(df[\"order_date_only\"])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74d560-6e33-4e01-b3eb-7465f0d73e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure order_processing_time exists\n",
    "if \"order_processing_time\" not in df.columns:\n",
    "    df[\"order_processing_time\"] = (pd.to_datetime(df[\"shipping_date_only\"]) - pd.to_datetime(df[\"order_date_only\"])).dt.days\n",
    "\n",
    "# Boxplot to visualize processing time by delay status\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=df[\"label\"], y=df[\"order_processing_time\"])\n",
    "plt.title(\"Order Processing Time vs Shipping Delay Status\")\n",
    "plt.xlabel(\"Delivery Status\")\n",
    "plt.ylabel(\"Order Processing Time (days)\")\n",
    "plt.show()\n",
    "\n",
    "# ANOVA test\n",
    "groups = [df[df[\"label\"] == lbl][\"order_processing_time\"].dropna() for lbl in df[\"label\"].unique()]\n",
    "anova_result = f_oneway(*groups)\n",
    "\n",
    "print(f\"ANOVA Test Results for Order Processing Time vs Shipping Delays\")\n",
    "print(f\"F-Statistic: {anova_result.statistic:.4f}, P-Value: {anova_result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0f12f-428f-4fbd-b083-20092369d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table\n",
    "crosstab = pd.crosstab(df[\"label\"], df[\"shipping_mode\"])\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "print(f\"Chi-Square Test for Shipping Method vs Shipping Delay\")\n",
    "print(f\"Chi-Square Statistic: {chi2:.4f}, P-Value: {p:.4f}\")\n",
    "\n",
    "# Visualize with heatmap\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(crosstab, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(\"Shipping Method vs Delivery Status\")\n",
    "plt.xlabel(\"Shipping Method\")\n",
    "plt.ylabel(\"Delivery Status\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd85aa0-4197-4bbf-8913-3126e17b8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'order_date' is in datetime format\n",
    "data_log_2['order_date'] = pd.to_datetime(data_log_2['order_date'], errors='coerce')\n",
    "\n",
    "# Copy the dataset and extract the year from 'order_date'\n",
    "temp = data_log_2.copy()\n",
    "temp['year'] = temp['order_date'].dt.year\n",
    "\n",
    "# Define categorical and numerical columns for analysis\n",
    "categories = ['market', 'customer_segment', 'department_id', 'category_id']\n",
    "numeric_metrics = ['order_item_quantity', 'sales', 'profit_per_order']\n",
    "\n",
    "for category in categories:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a row of 3 subplots\n",
    "    \n",
    "    if category in ['department_id', 'category_id']:\n",
    "        for idx, metric in enumerate(numeric_metrics):\n",
    "            # Find the top 4 departments/categories based on total metric value\n",
    "            top_values = temp.groupby(category)[metric].sum().nlargest(4).index\n",
    "            \n",
    "            # Filter dataset to include only top 4 departments/categories\n",
    "            filtered_temp = temp[temp[category].isin(top_values)]\n",
    "            \n",
    "            # Group by category and year, then plot the data\n",
    "            grouped_data = filtered_temp.groupby([category, 'year'])[metric].sum().unstack(category)\n",
    "            grouped_data.plot(kind='bar', ax=axes[idx])\n",
    "            \n",
    "            axes[idx].set_title(f'Yearly {metric} for Top 4 {category}s')\n",
    "            axes[idx].set_ylabel(f'Total {metric}')\n",
    "    \n",
    "    else:\n",
    "        for idx, metric in enumerate(numeric_metrics):\n",
    "            # Group by category and year, then plot the data\n",
    "            grouped_data = temp.groupby([category, 'year'])[metric].sum().unstack(category)\n",
    "            grouped_data.plot(kind='bar', ax=axes[idx])\n",
    "            \n",
    "            axes[idx].set_title(f'Yearly {metric} per {category}')\n",
    "            axes[idx].set_ylabel(f'Total {metric}')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c15d9-3367-4078-aa86-8a96b00faec0",
   "metadata": {},
   "source": [
    "## Trend Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f796f-77ce-4806-b7d8-f2f11eb8e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delayed According to Order Status\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Pie chart: Percentage of Delayed Orders\n",
    "delayed_counts = data_log_2.groupby(\"label\")[\"sales\"].count().sort_values(ascending=False)\n",
    "ax[0].pie(delayed_counts, labels=delayed_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('Blues'))\n",
    "ax[0].set_title(\"Percentage of Delayed Orders\")\n",
    "\n",
    "delayed_counts_df = data_log_2.groupby([\"order_status\",\"label\"])[[\"sales\"]].count().reset_index()\n",
    "# Count plot: Delay According to Order Status\n",
    "sns.barplot(\n",
    "    data=delayed_counts_df,  # \n",
    "    y=\"order_status\",\n",
    "    x=\"sales\",\n",
    "    hue=\"label\",\n",
    "    palette=\"tab20\",\n",
    "    ax=ax[1],\n",
    "    order=[\"COMPLETE\", \"PENDING_PAYMENT\", \"PROCESSING\", \"PENDING\", \"CLOSED\", \"ON_HOLD\", \"PAYMENT_REVIEW\"],\n",
    "    #h=[\"Early Delivery\", \"On-time Delivery\", \"Delayed Delivery\"],\n",
    "    width=0.6\n",
    ").set(ylabel=\"\", title=\"Delay According to Order Status\")\n",
    "\n",
    "# Adjust y-axis font size\n",
    "ax[1].tick_params(axis=\"y\", labelsize=6)\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.savefig(\"percentage_of_delayed_orders.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f7fa1-7d86-4f40-a9c8-a8aa1dd36a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_2.groupby([\"order_status\", \"label\"])[[\"sales\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709bc05-593d-4469-9b8d-ceeed1e7d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delayed_orders = data_log_2.shape[0] * 0.577\n",
    "\n",
    "count_delayed_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11d729-941c-4645-8182-2eefb6dc7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with \"NaT\" values in order_date and shipping_date column\n",
    "data_log_2 = data_log_2.dropna(subset=[\"order_date\", \"shipping_date\"])\n",
    "\n",
    "\n",
    "# Calculating Shipping delays\n",
    "# Convert order_date and shipping_date to datetime objects\n",
    "data_log_2['order_date'] = pd.to_datetime(data_log_2['order_date'], errors=\"coerce\", utc=True)\n",
    "data_log_2['shipping_date'] = pd.to_datetime(data_log_2['shipping_date'], errors=\"coerce\", utc=True)\n",
    "\n",
    "clean_data = data_log_2.dropna(subset=[\"order_date\", \"shipping_date\"]) # To prevent the error that would otherwise happen in the next step\n",
    "\n",
    "# Calculate the delay as the difference between shipping and order dates\n",
    "clean_data.loc[:, ['delay_days']] = (clean_data['shipping_date'] - clean_data['order_date']).dt.days\n",
    "\n",
    "# Filter rows where delay_days is greater than a threshold\n",
    "delayed_orders = clean_data[\n",
    "    (clean_data['delay_days'] > 7) & \n",
    "    (clean_data[\"label\"] == \"Delayed Delivery\") & \n",
    "    (clean_data[\"order_status\"] != \"PENDING_PAYMENT\") & \n",
    "    (clean_data[\"order_status\"] != \"PENDING\")\n",
    "]\n",
    "\n",
    "sample = delayed_orders[[\"delay_days\", \"product_name\", \"order_status\", \"order_country\", \"label\"]]\n",
    "\n",
    "count_treatable_delays = sample.shape[0]\n",
    "\n",
    "display(count_treatable_delays)\n",
    "\n",
    "percent_treatable_delays = (sample.shape[0] / data_log_2.shape[0]) * 100\n",
    "\n",
    "display(percent_treatable_delays)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66242366-0d9a-464e-b2b1-c2a6c5bdb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group delays by regions, cities, or countries\n",
    "\n",
    "delay_by_country = delayed_orders.groupby('order_country')['delay_days'].median().sort_values(ascending=False)\n",
    "\n",
    "display(delay_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e8394-c822-4e2f-bf1a-90e8d1a2c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shipping days for delayed products\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  \n",
    "\n",
    "sns.barplot(\n",
    "    y=delay_by_country[:10].index, \n",
    "    x=delay_by_country[:10].values, \n",
    "    palette=\"Blues_r\",\n",
    "    ax=ax  # \n",
    ").set(\n",
    "    ylabel=\"Order Country\", \n",
    "    xlabel=\"Average Number of Shipping Days\", \n",
    "    title=\"Top 10 Countries With Longest Average Shipping (Delayed Products)\"\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f92e6-4e9d-4711-ab84-7d71ba084898",
   "metadata": {},
   "source": [
    "## Modelling Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee621e-6e95-4e7b-a8fe-52327c970f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import  fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # Base plotting library\n",
    "import seaborn as sns # Main plotting library\n",
    "import geopandas as gpd\n",
    "import folium # Mapping library\n",
    "from IPython.display import display, IFrame # To display maps as cell outputs\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.model_selection import KFold # For dividing the dataset\n",
    "from sklearn.preprocessing import StandardScaler # For scaling features\n",
    "from sklearn.preprocessing import OneHotEncoder # For hot encoding data\n",
    "from sklearn.preprocessing import LabelEncoder # For simpler data encoding\n",
    "from sklearn.utils.class_weight import compute_class_weight # For dealing with different sizes of classification categories\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # For cross-validation evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b9205-9e4f-4421-ac42-d0191ce6f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb9e81-7d07-4daa-8748-04bab85c2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76f83c-fe18-44fd-8981-a452810d5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values and unwanted columns\n",
    "data = clean_data.dropna(axis=1, how=\"any\")[[\"order_country\", \"order_status\", \"shipping_mode\", \"payment_type\", \"label\", \"department_name\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9067f79-f825-45e9-872e-68ee4da39944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Handle categorical variables by encoding them\n",
    "onehot = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = onehot.fit_transform(X.select_dtypes(include=['object']))\n",
    "X_numeric = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Combine numerical features with one-hot encoded categorical features\n",
    "X = np.hstack((X_numeric, X_encoded))\n",
    "\n",
    "\n",
    "# Encoding y explicitly\n",
    "le_label = LabelEncoder()\n",
    "y = le_label.fit_transform(y)\n",
    "\n",
    "# Computing class weights from the classification column\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "## Changing the weights to a dictionary to be able to be fed to the model\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))} \n",
    "display(class_weights)\n",
    "\n",
    "# Normalize the numerical features (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Creating KFolds for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a82b0-0b94-44db-aeae-29202932156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    \n",
    "    # Defining the model\n",
    "    model = Sequential([\n",
    "    Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.01), input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(384, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # Multiclass Classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val), verbose=0, class_weight=class_weights)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_prob = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_val, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    \n",
    "    # Store the metrics for this fold\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Average Accuracy: {avg_accuracy * 100:.2f}%\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e523-3638-4dc4-a429-c23a2c476d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "# Define a function that builds the model with hyperparameters\n",
    "def create_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(units=hp_units, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # Tune dropout rate\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(rate=hp_dropout))\n",
    "    \n",
    "    # Tune number of hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 8)):  # Between 1 to 4 hidden layers\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(3, activation='softmax'))  # multiclass classification\n",
    "    \n",
    "    # Tune the learning rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    create_model,\n",
    "    objective='val_accuracy',  # Optimize for validation accuracy\n",
    "    max_trials=5,  # Number of different models to try\n",
    "    executions_per_trial=3,  # Number of times to train each model for robustness\n",
    "    directory='my_dir',  # Directory to save logs and results\n",
    "    project_name='tuning_delay_prediction'\n",
    ")\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Perform the hyper-parameter search\n",
    "    tuner.search(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Retrieve the best model and hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    # Print the best hyper-parameters\n",
    "    print(f\"The optimal number of units in the first hidden layer is {best_hps.get('units')}\")\n",
    "    print(f\"The optimal learning rate is {best_hps.get('learning_rate')}\")\n",
    "    \n",
    "    # Build and train the model using the best hyper-parameters\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val),  verbose=0, class_weight=class_weights)\n",
    "    \n",
    "#     model = create_model()\n",
    "#     history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val), verbose=0, class_weight=class_weights)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_prob = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_val, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    \n",
    "    # Store the metrics for this fold\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Average Accuracy: {avg_accuracy * 100:.2f}%\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ab7ac-9828-453d-9da2-f8880dc09cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "model1 = DecisionTreeClassifier(ccp_alpha=0.001, class_weight='balanced')\n",
    "model1.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Train Accuracy, Decision Tree\",model1.score(xtrain, ytrain))\n",
    "print(\"Test Accuracy, Decision Tree\",model1.score(xtest, ytest))\n",
    "print(\"DecisionTreeClassifier\")\n",
    "print(classification_report(ytrain, model1.predict(xtrain)))\n",
    "print(classification_report(ytest, model1.predict(xtest)))\n",
    "\n",
    "# RandomForestClassifier\n",
    "model2 = RandomForestClassifier(n_estimators=100, ccp_alpha=0.01, class_weight='balanced')\n",
    "model2.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Train Accuracy, RandomForest\",model2.score(xtrain, ytrain))\n",
    "print(\"Test Accuracy, RandomForest\",model2.score(xtest, ytest))\n",
    "print(\"RandomForestClassifier\")\n",
    "print(classification_report(ytrain, model2.predict(xtrain)))\n",
    "print(classification_report(ytest, model2.predict(xtest)))\n",
    "\n",
    "# AdaBoostClassifier\n",
    "model3 = AdaBoostClassifier(n_estimators=50, algorithm='SAMME')\n",
    "model3.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Train Accuracy, AdaBoost\", model3.score(xtrain, ytrain))\n",
    "print(\"Test Accuracy, AdaBoost\", model3.score(xtest, ytest))\n",
    "print(\"AdaBoostClassifier\")\n",
    "print(classification_report(ytrain, model3.predict(xtrain)))\n",
    "print(classification_report(ytest, model3.predict(xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fcea1-5a6e-43a8-816e-2c075bdda81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_venv",
   "language": "python",
   "name": "project1_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
